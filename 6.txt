import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

maze = np.array([
    [0, 0, 0, 1],
    [0, 1, 0, 1],
    [0, 0, 0, 0],
    [2, 1, 3, 0]
])

#0 represents free spaces. 1 represents walls or obstacles. 2 is the start point. 3 is the goal.

# Maze dimensions
rows, cols = maze.shape

# Define the start and goal positions
start = (3, 0)
goal = (3, 2)

# Possible actions (up, down, left, right)
actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]

# Parameters
alpha = 0.1    # Learning rate
gamma = 0.9    # Discount factor
epsilon = 0.1  # Exploration rate

# Q-table (Initialize Q-values for each state-action pair)
Q = np.zeros((rows, cols, len(actions)))

# Function to check if a position is valid
def is_valid(pos):
    return 0 <= pos[0] < rows and 0 <= pos[1] < cols and maze[pos] != 1

def choose_action(state):
    if np.random.rand() < epsilon:
        return np.random.choice(len(actions))  # Explore
    else:
        return np.argmax(Q[state[0], state[1], :])  # Exploit

def update_q(state, action, reward, next_state):
    old_q = Q[state[0], state[1], action]
    next_max_q = np.max(Q[next_state[0], next_state[1], :])
    Q[state[0], state[1], action] = old_q + alpha * (reward + gamma * next_max_q - old_q)

# Training the agent
def train_agent(episodes=1000):
    for _ in range(episodes):
        state = start
        while state != goal:
            action = choose_action(state)
            next_state = (state[0] + actions[action][0], state[1] + actions[action][1])

            if not is_valid(next_state):
                next_state = state  # If the action leads to an invalid state, stay in the same place
                reward = -1  # Penalty for hitting a wall
            elif next_state == goal:
                reward = 10  # Reward for reaching the goal
            else:
                reward = -0.1  # Small negative reward for each step taken

            update_q(state, action, reward, next_state)
            state = next_state

# Train the agent
train_agent()

def plot_maze():
    path = np.zeros((rows, cols))
    state = start
    path[state] = 2  # Mark start
    while state != goal:
        action = np.argmax(Q[state[0], state[1], :])
        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])
        if not is_valid(next_state) or state == next_state:
            break  # Stop if stuck
        state = next_state
        path[state] = 3  # Mark path
    path[goal] = 4  # Mark goal
    
    plt.imshow(path, cmap='coolwarm', interpolation='nearest')
    plt.title('Agent Path through the Maze')
    plt.show()

# Plot the maze with the agent's path
plot_maze()

# Visualizing the learned path
def visualize_path():
    state = start
    path = [state]  # Initialize path with the start state
    while state != goal:
        action = np.argmax(Q[state[0], state[1], :])
        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])
        if not is_valid(next_state) or state == next_state:
            break  # Stop if stuck
        state = next_state
        path.append(state)  # Append the next state to the path
    if state == goal:
        print("Path taken by the agent (from start to goal):")
        print(" -> ".join(str(step) for step in path))
    else:
        print("The agent could not find a path to the goal.")

# Visualize the path
visualize_path()
